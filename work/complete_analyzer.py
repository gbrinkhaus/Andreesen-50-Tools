#!/usr/bin/env python3
"""
Complete Analyzer - Creates ONE CSV with 3 rows per tool:
1. Original data
2. Link validation results
3. Content analysis results
"""

import csv
from pathlib import Path
from typing import Tuple
import requests
import time
from bs4 import BeautifulSoup
import undetected_chromedriver as uc
import warnings

warnings.filterwarnings("ignore")


class CompleteAnalyzer:
    def find_alternative_url(self, url: str) -> str:
        """Try to find a likely correct page if the original link is not valid."""
        import urllib.parse
        import requests
        from bs4 import BeautifulSoup

        # Try searching for the domain root
        parsed = urllib.parse.urlparse(url)
        if parsed.netloc:
            root_url = f"{parsed.scheme}://{parsed.netloc}"
            try:
                resp = requests.get(root_url, timeout=5)
                if resp.status_code == 200:
                    return root_url
            except Exception:
                pass

        # Try a simple Google search (if allowed)
        # (This is a placeholder, as real Google search requires API or scraping)
        # Could use DuckDuckGo or Bing API for production
        # For now, just return empty string
        return ""

    def __init__(
        self,
        csv_path: str,
        output_path: str = None,
        ollama_url: str = "http://localhost:11434",
        model: str = "gpt-oss:20b",
    ):
        # In-memory cache for fetched content
        self._content_cache = {}
        # Ensure cache directory exists
        self.cache_dir = Path("work/output/cache")
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        """
        Initialize analyzer

        Args:
            csv_path: Path to CSV file
            output_path: Path for output CSV (if None, creates _COMPLETE.csv)
            ollama_url: Ollama service URL
            model: Model to use
        """
        self.csv_path = Path(csv_path)
        self.output_path = (
            Path(output_path)
            if output_path
            else Path("work/output") / f"{self.csv_path.stem}_COMPLETE.csv"
        )
        self.ollama_url = ollama_url
        self.model = model

        self.session = requests.Session()
        self.session.headers.update({"User-Agent": "Mozilla/5.0"})

        self.link_columns = [
            "Homepage",
            "Privacy/Legal Link",
            "DSGVO/GDPR Link",
            "Storage/Hosting Link",
            "DPA/AVV Link",
        ]

        # Setup Selenium Chrome driver
        self.driver = None
        self._init_driver()

        self._verify_ollama()

    def _init_driver(self):
        """Initialize undetected Chrome driver to bypass Cloudflare"""
        try:
            self.driver = uc.Chrome(version_main=None, suppress_welcome=True)
            print("‚úÖ Undetected Chrome driver initialized")
        except Exception as e:
            print(f"‚ö†Ô∏è  Driver init failed: {e}")

    def _verify_ollama(self):
        """Verify Ollama is running"""
        try:
            response = requests.get(f"{self.ollama_url}/api/tags", timeout=5)
            if response.status_code == 200:
                print("‚úÖ Ollama connected (model: {})".format(self.model))
                return True
        except Exception as e:
            print(f"‚ùå Ollama error: {e}")
            return False

    def check_link(self, url: str) -> str:
        """Check if link is valid using browser"""
        if not url or not url.strip():
            return "‚ö†Ô∏è Empty"

        try:
            # Try with browser first (handles Cloudflare)
            if self.driver:
                try:
                    self.driver.get(url)
                    time.sleep(1)

                    # Check if we got a real page
                    page_source = self.driver.page_source
                    if page_source and len(page_source) > 500:
                        return "‚úÖ Valid (page loaded)"
                except Exception:
                    pass

            # Fallback to HTTP HEAD
            response = self.session.head(url, timeout=10, allow_redirects=True)

            if 200 <= response.status_code < 300:
                return "‚úÖ Valid (HTTP {})".format(response.status_code)
            elif response.status_code == 403:
                return "‚úÖ Valid (HTTP 403 - blocked)"
            elif response.status_code == 429:
                return "‚úÖ Valid (rate limited)"
            elif 300 <= response.status_code < 400:
                return "‚úÖ Valid (redirect {})".format(response.status_code)
            elif response.status_code == 404:
                alt = self.find_alternative_url(url)
                if alt:
                    return f"‚ùå Not found (HTTP 404) ‚Äì found {alt}"
                else:
                    return "‚ùå Not found (HTTP 404)"
            elif response.status_code == 410:
                alt = self.find_alternative_url(url)
                if alt:
                    return f"‚ùå Gone (HTTP 410) ‚Äì found {alt}"
                else:
                    return "‚ùå Gone (HTTP 410)"
            elif response.status_code >= 500:
                return "‚ö†Ô∏è Server error (HTTP {})".format(response.status_code)
            else:
                return "‚ùå Error (HTTP {})".format(response.status_code)

        except requests.exceptions.Timeout:
            return "‚ùå Timeout"
        except requests.exceptions.ConnectionError:
            return "‚ùå No connection"
        except Exception as e:
            return "‚ùå Error: {}".format(str(e)[:15])

    def fetch_content(self, url: str) -> str:
        """Fetch page content using undetected Chrome (bypasses Cloudflare)"""
        # Use a filename-safe hash for the cache file
        import hashlib

        url_hash = hashlib.sha256(url.encode("utf-8")).hexdigest()
        cache_file = self.cache_dir / f"{url_hash}.txt"

        # 1. Check file cache
        if cache_file.exists():
            try:
                content = cache_file.read_text(encoding="utf-8")
                if content:
                    self._content_cache[url] = content
                    return content
            except Exception:
                pass

        # 2. Check in-memory cache
        if url in self._content_cache:
            return self._content_cache[url]

        # 3. Fetch if not cached
        if not self.driver:
            return ""

        try:
            self.driver.get(url)
            # Wait for page to load
            time.sleep(3)

            # Get page source
            page_source = self.driver.page_source

            if page_source and len(page_source) > 100:
                soup = BeautifulSoup(page_source, "html.parser")

                # Remove script and style tags
                for script in soup(["script", "style"]):
                    script.decompose()

                # Get text
                text = soup.get_text()

                # Clean up whitespace
                lines = (line.strip() for line in text.splitlines())
                chunks = (
                    phrase.strip() for line in lines for phrase in line.split("  ")
                )
                text = " ".join(chunk for chunk in chunks if chunk)

                if text and len(text) > 50:
                    # Save to in-memory cache
                    self._content_cache[url] = text
                    # Save to file cache
                    try:
                        cache_file.write_text(text, encoding="utf-8")
                    except Exception:
                        pass
                    return text
        except Exception:
            pass
        self._content_cache[url] = ""
        try:
            cache_file.write_text("", encoding="utf-8")
        except Exception:
            pass
        return ""

    def analyze_with_ollama(self, link_type: str, content: str) -> str:
        """Use Ollama to analyze if content matches link type"""
        if not content:
            return "‚ùå No content"

        prompts = {
            "Homepage": "Is this the main website homepage?",
            "Privacy/Legal Link": "Does this contain privacy policy or legal terms?",
            "DSGVO/GDPR Link": (
                "Does this page mention GDPR, DSGVO, EU data protection, or any legal basis for international data transfers under EU law (such as Article 45 GDPR, Article 46 GDPR, adequacy decisions, or standard contractual clauses)? "
                "Answer YES if there is any reference to GDPR, DSGVO, EU data protection, or legal mechanisms for data transfers (including adequacy decisions or standard contractual clauses)."
            ),
            "Storage/Hosting Link": "Does this describe data storage, hosting, or security?",
            "DPA/AVV Link": "Does this contain a Data Processing Agreement or DPA?",
        }

        chunk_size = 4000
        chunks = [
            content[i : i + chunk_size] for i in range(0, len(content), chunk_size)
        ]
        any_yes = False
        for idx, chunk in enumerate(chunks):
            prompt = f"""Analyze: {prompts.get(link_type, "What is this?")}

CONTENT:
{chunk}

Answer ONLY with: YES or NO (one word)."""
            print(
                f"\n--- OLLAMA PROMPT for {link_type} (chunk {idx + 1}/{len(chunks)}) ---\nQUESTION: {prompts.get(link_type, 'What is this?')}\n--- END PROMPT ---\n"
            )
            try:
                response = requests.post(
                    f"{self.ollama_url}/api/generate",
                    json={
                        "model": self.model,
                        "prompt": prompt,
                        "stream": False,
                        "temperature": 0.1,
                    },
                    timeout=60,
                )
                if response.status_code == 200:
                    result = response.json().get("response", "").strip().upper()
                    if "YES" in result:
                        any_yes = True
                        break
                else:
                    print(f"‚ùå Ollama error: {response.status_code}")
            except Exception as e:
                print(f"‚ùå Ollama error: {e}")
        if any_yes:
            return "‚úÖ Yes"
        else:
            return "‚ùå No"

        print(
            f"\n--- OLLAMA PROMPT for {link_type} ---\nQUESTION: {prompts.get(link_type, 'What is this?')}\n--- END PROMPT ---\n"
        )

        try:
            response = requests.post(
                f"{self.ollama_url}/api/generate",
                json={
                    "model": self.model,
                    "prompt": prompt,
                    "stream": False,
                    "temperature": 0.1,
                },
                timeout=60,
            )

            if response.status_code == 200:
                result = response.json().get("response", "").strip().upper()

                # Format with consistent icons
                if "YES" in result:
                    return f"‚úÖ Yes"
                elif "NO" in result:
                    return f"‚ùå No"
                else:
                    return f"‚ö†Ô∏è Unclear: {result[:30]}"
        except requests.exceptions.Timeout:
            return "‚ö†Ô∏è Timeout"
        except Exception as e:
            return f"‚ö†Ô∏è Error"

        return "‚ö†Ô∏è No response"

    def process(self, start_line: int = 1, end_line: int = None):
        """
        Process CSV: creates 3 rows per tool
        """
        # Read CSV
        print("üìÇ Reading CSV...")
        rows = []
        fieldnames = []

        with open(self.csv_path, "r", encoding="utf-8") as f:
            lines = f.readlines()
            for i, line in enumerate(lines):
                if "App name" in line or "Kategorie" in line:
                    f_temp = open(self.csv_path, "r", encoding="utf-8")
                    reader = csv.DictReader(f_temp.readlines()[i:], delimiter=";")
                    fieldnames = reader.fieldnames or []
                    rows = list(reader)
                    f_temp.close()
                    break

        print(f"‚úÖ Loaded {len(rows)} rows")

        # Determine range
        total = len(rows)
        end_line = end_line if end_line else total
        start_idx = start_line - 1
        end_idx = min(end_line, total)

        print(f"\n{'=' * 80}")
        print(f"üîç CREATING COMPLETE ANALYSIS (lines {start_line} to {end_idx})")
        print(f"{'=' * 80}\n")

        output_rows = []

        # Add rows before processing range
        output_rows.extend(rows[:start_idx])

        # Process each row

        for idx in range(start_idx, end_idx):
            original_row = rows[idx]
            line_num = idx + 1

            tool_name = original_row.get("App name", f"Tool #{line_num}")
            print(f"üìç Line {line_num}: {tool_name}")

            # Add original row
            output_rows.append(original_row)

            # Create link check row
            link_check_row = {key: "" for key in fieldnames}
            link_check_row["App name"] = "[LINK CHECK]"

            for link_col in self.link_columns:
                if link_col not in fieldnames:
                    continue

                url = original_row.get(link_col, "").strip()
                result = self.check_link(url)
                link_check_row[link_col] = result
                print(f"  {result} {link_col}")

                time.sleep(0.3)

            output_rows.append(link_check_row)

            # Create content analysis row
            analysis_row = {key: "" for key in fieldnames}
            analysis_row["App name"] = "[CONTENT ANALYSIS]"

            for link_col in self.link_columns:
                if link_col not in fieldnames:
                    continue

                url = original_row.get(link_col, "").strip()

                if not url:
                    analysis_row[link_col] = "‚ö†Ô∏è No URL"
                else:
                    print(f"  üîç Analyzing {link_col}...")
                    content = self.fetch_content(url)

                    if not content:
                        analysis_row[link_col] = "‚ùå No content"
                    else:
                        analysis = self.analyze_with_ollama(link_col, content)
                        analysis_row[link_col] = analysis
                        print(f"     {analysis}")

                time.sleep(1)

            output_rows.append(analysis_row)

            # Write results after each tool
            self.output_path.parent.mkdir(parents=True, exist_ok=True)
            with open(self.output_path, "w", encoding="utf-8", newline="") as f:
                writer = csv.DictWriter(
                    f, fieldnames=fieldnames, delimiter=";", extrasaction="ignore"
                )
                writer.writeheader()
                writer.writerows(output_rows)

        # Add rows after processing range
        if end_idx < len(rows):
            output_rows.extend(rows[end_idx:])

        # Write output
        print(f"\n{'=' * 80}")
        print("üíæ Writing output CSV...")

        # Ensure output directory exists before writing
        self.output_path.parent.mkdir(parents=True, exist_ok=True)
        with open(self.output_path, "w", encoding="utf-8", newline="") as f:
            writer = csv.DictWriter(
                f, fieldnames=fieldnames, delimiter=";", extrasaction="ignore"
            )
            writer.writeheader()
            writer.writerows(output_rows)

        print(f"‚úÖ Output: {self.output_path}")
        print(f"\nStructure per tool:")
        print(f"  Row 1: Original data")
        print(f"  Row 2: [LINK CHECK] - Link validation results")
        print(f"  Row 3: [CONTENT ANALYSIS] - Ollama analysis results")


def main():
    """Command line interface"""
    import sys

    if len(sys.argv) < 2:
        print("Usage: python complete_analyzer.py <csv_file> [start_line] [end_line]")
        print(
            "Example: python complete_analyzer.py 'Andreesen Tools 50 - UPDATED.csv' 1 2"
        )
        sys.exit(1)

    csv_file = sys.argv[1]
    start_line = int(sys.argv[2]) if len(sys.argv) > 2 else 1
    end_line = int(sys.argv[3]) if len(sys.argv) > 3 else None

    analyzer = CompleteAnalyzer(csv_file)
    try:
        analyzer.process(start_line=start_line, end_line=end_line)
    finally:
        if analyzer.driver:
            analyzer.driver.quit()


if __name__ == "__main__":
    main()
